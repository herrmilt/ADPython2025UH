{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to create a model to distinguish between circles and squares in images. First, I create a dataset containing images of both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "    def __init__(self, num_samples, image_size=28):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.data, self.labels = self.generate_data(num_samples, image_size)\n",
    "        \n",
    "    def generate_data(self, num_samples, image_size):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for _ in range(num_samples):\n",
    "            label = np.random.randint(0, 2)\n",
    "            image = np.zeros((image_size, image_size), dtype=np.float32)\n",
    "            if label == 0:\n",
    "                # Draw a square\n",
    "                side = np.random.randint(5, image_size // 2)\n",
    "                top_left_x = np.random.randint(0, image_size - side)\n",
    "                top_left_y = np.random.randint(0, image_size - side)\n",
    "                image[top_left_x:top_left_x+side, top_left_y:top_left_y+side] = 1.0\n",
    "            else:\n",
    "                # Draw a circle\n",
    "                radius = np.random.randint(5, image_size // 4)\n",
    "                center_x = np.random.randint(radius, image_size - radius)\n",
    "                center_y = np.random.randint(radius, image_size - radius)\n",
    "                y, x = np.ogrid[:image_size, :image_size]\n",
    "                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "                image[mask] = 1.0\n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "        return torch.tensor(data).unsqueeze(1), torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset and dataloaders\n",
    "train_dataset = ShapesDataset(num_samples=1000)\n",
    "test_dataset = ShapesDataset(num_samples=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets display some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    image, label = train_dataset[i]\n",
    "    axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {'Square' if label.item() == 0 else 'Circle'}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a network using pixels as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]\n",
    "\n",
    "print(\"Original\", image.shape)\n",
    "# The first step is flattening the image to enter the networn\n",
    "nn.Flatten(1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(784, 20),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with this solution:\n",
    "- The interaction of every pair of pixels is taken into account in the linear layer. This creates a very large number of parameters to learn.\n",
    "- The model is very sensitive to image translations, because each pixel is considered a feature.\n",
    "- The locality is completelly missed. Borders, for example, cannot be found.\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "A convolution is a mathematical operation that combines two sets of information.\n",
    "\n",
    "In computer vision, a convolution transform an image using a kernel. Convolution transforms pixels into features, that takes into account in a particular way the relations of each pixel with its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_convolution(input_data, kernel):\n",
    "    # Get dimensions\n",
    "    input_h, input_w = input_data.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_h = input_h - kernel_h + 1\n",
    "    output_w = input_w - kernel_w + 1\n",
    "\n",
    "    # Initialize the output\n",
    "    output = torch.zeros((output_h, output_w), dtype=torch.float32, device=input_data.device)\n",
    "\n",
    "    # Perform the convolution operation\n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            # Extract the region of interest\n",
    "            region = input_data[i:i+kernel_h, j:j+kernel_w]\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            output[i, j] = torch.sum(region * kernel)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image, title='',*, cmap='gray', **kwargs):\n",
    "    plt.imshow(image, cmap=cmap, **kwargs)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "image = np.zeros((10, 10), dtype=np.float32)\n",
    "image[3:7, 3:7] = 1.0\n",
    "\n",
    "# Display the original image\n",
    "display_image(image, title='Original Image')\n",
    "\n",
    "# Convert the image to a PyTorch tensor and add a batch dimension and a channel dimension\n",
    "image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_by_kernel(image_tensor, kernel, title):\n",
    "    output = F.conv2d(image_tensor, kernel, padding='same').squeeze().numpy()\n",
    "    display_image(output, title)\n",
    "\n",
    "edge_detection_kernel = torch.tensor([[\n",
    "    [-1, -1, -1], \n",
    "    [-1, 8, -1], \n",
    "    [-1, -1, -1]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, edge_detection_kernel, 'Edge Detection Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edge_kernel = torch.tensor([[\n",
    "    [-1, 0, 1], \n",
    "    [-1, 0, 1], \n",
    "    [-1, 0, 1]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, vertical_edge_kernel, 'Horizontal Edge Detection Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_edge_kernel = torch.tensor([[\n",
    "    [1, 1, 1], \n",
    "    [0, 0, 0], \n",
    "    [-1, -1, -1]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, horizontal_edge_kernel, 'Horizontal Edge Detection Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_kernel = torch.tensor([[\n",
    "    [0, -1, 0], \n",
    "    [-1, 4, -1], \n",
    "    [0, -1, 0]]], dtype=torch.float32).unsqueeze(0)\n",
    "show_by_kernel(image_tensor, corner_kernel, 'Corners')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions find new features that contains information a pixel region, so now they can include information about borders.\n",
    "\n",
    "If we train a network using convoluted values, we can provide better information with less size:\n",
    "- Learn the kernels in the first layers\n",
    "\n",
    "Additionally, we would need to reduce the size of the images on each layer:\n",
    "- Reduce the dimensionality, so the number of parameters\n",
    "- Translation invariance, by summarizing feautures in regions\n",
    "- Hierarchical features, by producing more abstract features per layers\n",
    "- Noise reduction \n",
    "\n",
    "CNNs are neural networks that use convolutions to create higher-level features based on the input images:\n",
    "- In convolutional layers, the kernels are learnt\n",
    "- All operations are differenciable, so can be learned by gradient descent and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(3 * 3, 2)  # Adjust the dimensions accordingly\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.tanh(self.conv1(x)))\n",
    "        x = self.pool(self.tanh(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore now what are the convolutions learned by the model while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_1 = [img for img, cls in train_dataset if cls == 0][0]\n",
    "circle_1 = [img for img, cls in train_dataset if cls == 1][0]\n",
    "display_image(square_1.squeeze())\n",
    "display_image(circle_1.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.conv1(square_1).squeeze()\n",
    "    display_image(image)\n",
    "    image = model.conv1(circle_1).squeeze()\n",
    "    display_image(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply the pooling and the second convolution to both examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.conv2(model.pool(torch.tanh(model.conv1(square_1)))).squeeze()\n",
    "    display_image(image)\n",
    "    image = model.conv2(model.pool(torch.tanh(model.conv1(circle_1)))).squeeze()\n",
    "    display_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we apply the last pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image = model.pool(torch.tanh(model.conv2(\n",
    "        model.pool(torch.tanh(model.conv1(square_1)))))).squeeze()\n",
    "    display_image(image)\n",
    "    image = model.pool(torch.tanh(model.conv2(\n",
    "        model.pool(torch.tanh(model.conv1(circle_1)))))).squeeze()\n",
    "    display_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show the results for some of the circles and squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = [img for img, cls in train_dataset if cls == 0][:6]\n",
    "circles = [img for img, cls in train_dataset if cls == 1][:6]\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(5, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = model.pool(torch.tanh(model.conv2(\n",
    "            model.pool(torch.tanh(model.conv1(squares[i])))))).squeeze()\n",
    "        ax.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(5, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = model.pool(torch.tanh(model.conv2(\n",
    "            model.pool(torch.tanh(model.conv1(circles[i])))))).squeeze()\n",
    "        ax.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real, more complex applications, we usually performs different parallel convolutions to the same input. The result of each of the convolutions is stored in a different chanel of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single convolution kernel\n",
    "nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1).weight.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((10, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple convolution kernels\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1).weight.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((10, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multipe inputs channels, actual kernels have one more dimmension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=1).weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((10, 3, 28, 28))\n",
    "nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final case, multiple inputs, multiple outputs\n",
    "nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, padding=1).weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((10, 3, 28, 28))\n",
    "nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, padding=1)(image).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets explore the kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn((1, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=7)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "\n",
    "image = torch.randn((1, 1, 28, 28))\n",
    "nn.Conv2d(in_channels=1, out_channels=5, kernel_size=7, padding=3)(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding, auto\n",
    "\n",
    "image = torch.randn((1, 1, 28, 28))\n",
    "print(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding='same')(image).shape)\n",
    "print(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=7, padding='same')(image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets change the model to use multiple kernels per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(16 * 3 * 3, 2)  # out_channels * (28 // 3 //3) * (28 // 3 //3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.tanh(self.conv1(x)))\n",
    "        x = self.pool(self.tanh(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 3 * 3)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In color images, the input images has three channels, so we need to adjust the conv2d parameters.\n",
    "\n",
    "In this example, I will differentiate between two classes:\n",
    "- class 0: yellow squares\n",
    "- class 1: red squares and yellow circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels are R, G, B. yellow = R+G\n",
    "\n",
    "def create_square(image_size, color='rgb'):\n",
    "    image = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "    side = np.random.randint(5, image_size // 2)\n",
    "    top_left_x = np.random.randint(0, image_size - side)\n",
    "    top_left_y = np.random.randint(0, image_size - side)\n",
    "    for idx, c in enumerate('rgb'):\n",
    "        if c in color:\n",
    "            image[idx, top_left_x:top_left_x+side, top_left_y:top_left_y+side] = 1.0\n",
    "    return image\n",
    "\n",
    "def create_circle(image_size, color='rgb'):\n",
    "    image = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "    radius = np.random.randint(5, image_size // 4)\n",
    "    center_x = np.random.randint(radius, image_size - radius)\n",
    "    center_y = np.random.randint(radius, image_size - radius)\n",
    "    y, x = np.ogrid[:image_size, :image_size]\n",
    "    mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "    for idx, c in enumerate('rgb'):\n",
    "        if c in color:\n",
    "            image[idx, mask] = 1.0\n",
    "    return image\n",
    "\n",
    "class ShapesDataset2(Dataset):\n",
    "    def __init__(self, num_samples, image_size=28):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.data, self.labels = self.generate_data(num_samples, image_size)\n",
    "        \n",
    "    def generate_data(self, num_samples, image_size):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for _ in range(num_samples):\n",
    "            label = np.random.randint(0, 2)\n",
    "            # image = np.zeros((3, image_size, image_size), dtype=np.float32)\n",
    "            if label == 0:\n",
    "                # Draw a yellow square\n",
    "                image = create_square(image_size, 'rg')\n",
    "            else:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    image = create_circle(image_size, 'rg')\n",
    "                else:\n",
    "                    image = create_square(image_size, 'r')\n",
    "                    \n",
    "            data.append(image)\n",
    "            labels.append(label)\n",
    "        return torch.tensor(data), torch.tensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset and dataloaders\n",
    "train_dataset = ShapesDataset2(num_samples=1000)\n",
    "test_dataset = ShapesDataset2(num_samples=200)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some samples\n",
    "fig, axes = plt.subplots(3, 5, figsize=(10,6))\n",
    "for i in range(15):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    ax = axes[row, col]\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {'Class 1' if label.item() == 0 else 'Class 2'}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNColor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNColor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=2, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(2 * 3 * 3, 2)  # out_channels * (28 // 3 //3) * (28 // 3 //3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.tanh(self.conv1(x)))\n",
    "        x = self.pool(self.tanh(self.conv2(x)))\n",
    "        x = x.view(-1, 2 * 3* 3)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNNColor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_channels, in_channels, h, w\n",
    "model.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.conv1.weight[0])\n",
    "torch.sum(model.conv1.weight[0], dim=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.conv1.weight[1])\n",
    "torch.sum(model.conv1.weight[1], dim=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now explore the results of applying those convolutions to the three types of objects we have in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_color(image, title='',*, cmap='gray'):\n",
    "    plt.imshow(image.squeeze().permute(1, 2, 0).numpy(), cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = torch.tensor(create_square(28,'rg'))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,6))\n",
    "axes[0].imshow(obj.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "with torch.no_grad():\n",
    "    c1 = model.conv1(obj)\n",
    "axes[1].imshow(c1[0], cmap='gray')\n",
    "axes[2].imshow(c1[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = torch.tensor(create_square(28,'r'))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,6))\n",
    "axes[0].imshow(obj.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "with torch.no_grad():\n",
    "    c1 = model.conv1(obj)\n",
    "axes[1].imshow(c1[0], cmap='gray')\n",
    "axes[2].imshow(c1[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = torch.tensor(create_circle(28,'rg'))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,6))\n",
    "axes[0].imshow(obj.squeeze().permute(1, 2, 0).numpy(), cmap='gray')\n",
    "with torch.no_grad():\n",
    "    c1 = model.conv1(obj)\n",
    "axes[1].imshow(c1[0], cmap='gray')\n",
    "axes[2].imshow(c1[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the classes\n",
    "- class 0: yellow squares\n",
    "- class 1: red squares and yellow circles\n",
    "\n",
    "Here you can see that on red objects, no borders are extracte because it is not necesary in order to distinguish the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
