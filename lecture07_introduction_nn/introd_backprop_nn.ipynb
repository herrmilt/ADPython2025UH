{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7df9691",
   "metadata": {},
   "source": [
    "# Introduction to Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b075ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926fff1",
   "metadata": {},
   "source": [
    "## Manually calculating gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5e86e",
   "metadata": {},
   "source": [
    "Lets create a class for storing values, adding operations and parameters of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from value import Value, draw_dot\n",
    "    \n",
    "a = Value(2)\n",
    "b = Value(-3.0)\n",
    "d = (a*b)\n",
    "draw_dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852e0e0",
   "metadata": {},
   "source": [
    "Lets analyze the expression: L = (a*b+c)*f\n",
    "\n",
    "Lets add some labels to the nodes, in order to identify them with ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label='d'\n",
    "f = Value(-2, label='f')\n",
    "L = d*f; L.label='L'\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1190b2",
   "metadata": {},
   "source": [
    "You can see that we have a matematical expression linking L with four free parameters: a, b, c, and f. We are now to run backpropagation, trying to increase the value of L by changing the values of the free parameters.\n",
    "- For every single value we are going to calculate the derivative, using the chain rule. Using this, we will know how to change the values for increasing L \n",
    "\n",
    "In order to do so, we will add a property in Value to hold the derivative of L with respect to that value. We will name this property 'grad'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d50ba",
   "metadata": {},
   "source": [
    "We start back to front, manually. We started by L\n",
    "\n",
    "dL/dL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7abed6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L.grad = 1\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28c6d4",
   "metadata": {},
   "source": [
    " Now, lets calculate the derivatives with respect to f and d. Since L = f*d:\n",
    " \n",
    " dL/df = d\n",
    " \n",
    " dL/dd = f\n",
    " \n",
    " Lets check by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2+h, label='f')  # HERE    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d2961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    d.data += h                 # HERE\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since they are correct, we will set the values\n",
    "d.grad = f.data\n",
    "f.grad = d.data\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a97dd",
   "metadata": {},
   "source": [
    "Now, lets move to the previous values, e and c. We now need to calculate dL/de and dL/dc. From the chain rule we know that:\n",
    "\n",
    "dL/de = dL/dd*dd/de\n",
    "- We already calculated dL/dd, which is -2\n",
    "- Since d = e+c, dd/de=1\n",
    "- As a result **dL/de = -2 * 1 = -2**\n",
    "\n",
    "In a similar waw, **dL/dc = -2**\n",
    "\n",
    "**Note**: The + node only passes the gradient without modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c2953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After the check we know it is correct, so we update the .grad property\n",
    "e.grad = -2\n",
    "c.grad = -2\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e7ad2",
   "metadata": {},
   "source": [
    "Now, lets calculate the final one, the gradient for *a* and *b*, also using the chain rule.\n",
    "\n",
    "- dL/da = dL/de * de/da\n",
    "- dL/de is already known, -2\n",
    "- Since e = a*b, then de/da = b\n",
    "\n",
    "Finally, dL/da = -2 * -3 = 6\n",
    "\n",
    "Similarly, dL/db = dL/de*de/db = -2 * a = -2 * 2 = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df8508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since they are correct, we updated the .grad properties.\n",
    "a.grad = 6\n",
    "b.grad = -4\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557fba6",
   "metadata": {},
   "source": [
    "**Note**: There are some parameters that we can change, like a, b, c, and f, while the others are calculated, so cannot be changed.\n",
    "\n",
    "Now, let use the gradient in order to increase the value of L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, bg, cg, fg = a.grad, b.grad, c.grad, f.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_by_hand(h = 0.01):\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2 + ag * h, label='a')\n",
    "    b = Value(-3.0 + bg * h, label='b')\n",
    "    c = Value(10 + cg * h, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2 + fg * h, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L2 = L.data\n",
    "\n",
    "    print(L1, L2, L2-L1)\n",
    "    \n",
    "eval_by_hand(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677eff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_hand(-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af34270",
   "metadata": {},
   "source": [
    "Note, it is increased. This is the backpropagation algorithm in action! \n",
    "\n",
    "Lets move to something more complex, like a neuron.\n",
    "\n",
    "A neural network has:\n",
    "- Neurons\n",
    "    - Weights\n",
    "    - Bias (neuron default activation in absence of inputs)\n",
    "    - Activation function: \n",
    "        - Introduce nonlinearities, frequently squashing the neuron output\n",
    "        \n",
    "There are some common activation function, like the tanh and ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2437561",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "plt.plot(x, np.tanh(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec11d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.1)\n",
    "\n",
    "plt.plot(x, np.where(x > 0, x, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c99b45",
   "metadata": {},
   "source": [
    "Lets create a sistem a neuron and two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2969fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.7, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "\n",
    "draw_dot(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a30b9",
   "metadata": {},
   "source": [
    "Now evaluate the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d5af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ed602",
   "metadata": {},
   "source": [
    "Note than here the efect of tanh is minimal, but lets change the bias to 10 we would see the squashing efect of tanh. Now, lets run backpropagation on the neuron.\n",
    "\n",
    "**Note**. While training the neuron, the only parameters that we can change are the weights and biases, because the training examples are fixed.\n",
    "\n",
    "First, since do/do=1, lets set that in the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ff24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d428f",
   "metadata": {},
   "source": [
    "dtanh(x)/dx = 1 - tanh(x)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225551f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.grad = 1 - (o.data)**2\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd9c76",
   "metadata": {},
   "source": [
    "do/db = do/dn*dn/db = 0.5 * 1\n",
    "\n",
    "do/d(x1w1+x2w2) = do/dn * dn/d(x1w1+x2w2) = 0.5 * 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f902d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.grad = 0.5\n",
    "x1w1x2w2.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce22b8",
   "metadata": {},
   "source": [
    "do/dx2w2 = do/d(x1w1+x2w2) * d(x1w1+x2w2)/dx2w2\n",
    "    = 0.5 * 1 = 0.5\n",
    "    \n",
    "do/dx1w1 = do/d(x1w1+x2w2) * d(x1w1+x2w2)/dx1w1\n",
    "    = 0.5 * 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb7d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x2w2.grad = 0.5\n",
    "x1w1.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57ead2",
   "metadata": {},
   "source": [
    "do/dw1 = do/dx1w1 * dx1w1/dw1 = 0.5*x1 = 0.5*2 = 1\n",
    "\n",
    "do/dx1 = do/dx1w1 * dx1w1/dx1 = 0.5*w1 = 0.5*-3 = -1.5\n",
    "\n",
    "do/dw2 = do/dx2w2 * dx2w2/dw2 = 0.5*x2 = 0.5*0 = 0\n",
    "\n",
    "d0/dx2 = do/dx2w2 * dx2w2/dx2 = 0.5*w2 = 0.5*1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.grad = 1\n",
    "x1.grad = -1.5\n",
    "w2.grad = 0\n",
    "x2.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789dc939",
   "metadata": {},
   "source": [
    "Now, the parameters we can modify in order to increase the value of the function are the weights and biases.\n",
    "\n",
    "Lets modify them to icrease the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed59bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w1 = 1\n",
    "grad_w2 = 0\n",
    "grad_b = 0.5\n",
    "\n",
    "def modify(h):\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0 + h*grad_w1), Value(1.0+h*grad_w2)\n",
    "    b = Value(6.8812735870195432 + h*grad_b)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    return L1, L2, L2-L1\n",
    "\n",
    "print(modify(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da0c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(modify(-0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec758662",
   "metadata": {},
   "source": [
    "## AutoGrad\n",
    "\n",
    "You can see that applying backpropagation is very simple, but tedious. Lets move to automatically calculate the gradient (autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabb81e",
   "metadata": {},
   "source": [
    "Lets see it on a more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e5be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b; d.label='d'\n",
    "e = a + b; e.label='e'\n",
    "f = d * e; f.label='f'\n",
    "\n",
    "f.backward()\n",
    "draw_dot(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be192117",
   "metadata": {},
   "source": [
    "You can check by hand that everything is working perfectly. Lets add a final manual check!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    h = 0.001\n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L1 = f.data\n",
    "    \n",
    "    a = Value(-2.0+h, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L2 = f.data \n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    h = 0.001\n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L1 = f.data\n",
    "    \n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0+h, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L2 = f.data \n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "check()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
